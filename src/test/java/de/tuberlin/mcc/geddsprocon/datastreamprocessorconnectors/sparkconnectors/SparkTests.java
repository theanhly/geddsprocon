package de.tuberlin.mcc.geddsprocon.datastreamprocessorconnectors.sparkconnectors;

import de.tuberlin.mcc.geddsprocon.DSPConnectorFactory;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.serializer.KryoSerializer;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.receiver.Receiver;
import org.junit.Test;
import scala.Tuple2;

import java.util.Arrays;
import java.util.Iterator;

public class SparkTests {

    @Test
    public void sparkSourceTest() {
        try {
            SparkConf sparkConf = new SparkConf().setAppName("JavaCustomReceiver").setMaster("local[*]").set("spark.executor.memory","1g").set("spark.serializer", KryoSerializer.class.getName());
            JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, new Duration(5000));

            // Create an input stream with the custom receiver on target ip:port and count the
            // words in input stream of \n delimited text (eg. generated by 'nc')
            JavaReceiverInputDStream<String> lines =
                    ssc.receiverStream((Receiver)new DSPConnectorFactory<>().createSourceConnector(DSPConnectorFactory.DataStreamProcessors.SPARK, "localhost", 6555));

            //      Split each line into words
            JavaDStream<String> words = lines.flatMap(
                    (FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator()
            );

            //      Count each word in each batch
            JavaPairDStream<String, Integer> pairs = words.mapToPair(
                    (PairFunction<String, String, Integer>) s -> new Tuple2<>(s, 1)
            );


            //      Cumulate the sum from each batch
            JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey(
                    (Function2<Integer, Integer, Integer>) (i1, i2) -> i1 + i2
            );

            wordCounts.print();
            ssc.start();
            ssc.awaitTermination();
        } catch(Exception ex) {
            System.err.println(ex.toString());
        }
    }

    @Test
    public void sparkSinkTest() {
        try {
            SparkConf sparkConf = new SparkConf().setAppName("JavaCustomReceiver").setMaster("local[*]").set("spark.executor.memory","1g").set("spark.serializer", KryoSerializer.class.getName());
            JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, new Duration(5000));

            // Create an input stream with the custom receiver on target ip:port and count the
            // words in input stream of \n delimited text (eg. generated by 'nc')
            JavaReceiverInputDStream<String> lines =
                    ssc.receiverStream((Receiver)new DSPConnectorFactory<>().createSourceConnector(DSPConnectorFactory.DataStreamProcessors.SPARK, "localhost", 6555));

            //      Split each line into words
            JavaDStream<String> words = lines.flatMap(
                    (FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator()
            );

            //words.foreachRDD((VoidFunction)new DSPConnectorFactory<>().createSinkConnector(DSPConnectorFactory.DataStreamProcessors.SPARK, "localhost", 6556));

            //      Count each word in each batch
            JavaPairDStream<String, Integer> pairs = words.mapToPair(
                    (PairFunction<String, String, Integer>) s -> new Tuple2<>(s, 1)
            );


            //      Cumulate the sum from each batch
            JavaPairDStream<String, Integer> wordCounts = pairs.reduceByKey(
                    (Function2<Integer, Integer, Integer>) (i1, i2) -> i1 + i2
            );

            wordCounts.foreachRDD((VoidFunction)new DSPConnectorFactory<>().createSinkConnector(DSPConnectorFactory.DataStreamProcessors.SPARK, "localhost", 6556));

            wordCounts.print();
            ssc.start();
            ssc.awaitTermination();
        } catch(Exception ex) {
            System.err.println(ex.toString() + ex.getStackTrace());
        }
    }

}
